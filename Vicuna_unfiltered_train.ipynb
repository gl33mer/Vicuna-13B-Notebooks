{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuhausmatheus/vicuna/blob/main/Vicuna_unfiltered_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GNnhOCQE7sa"
      },
      "source": [
        "**You may encounter an error when installing flash-attn. I couldn't figure it out. Maybe you can.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h_MevKtB0dEw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!cd ~\n",
        "!git clone https://github.com/huggingface/transformers.git && cd transformers && git checkout cae78c46 && pip install .\n",
        "# Install fastchat\n",
        "!pip3 install --upgrade pip\n",
        "!git clone https://github.com/lm-sys/FastChat && cd FastChat && pip install -e .\n",
        "%pip install einops\n",
        "!mkdir checkpoints\n",
        "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl\n",
        "%pip install flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SNhHJFz-28c"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/download-model.py\n",
        "!mkdir models\n",
        "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V2_unfiltered_cleaned_split.json\n",
        "!python download-model.py decapoda-research/llama-13b-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIkKAeiqE7sb"
      },
      "source": [
        "**Manually edit tokenizer_config.json to: {\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\": 2048, \"tokenizer_class\": \"LlamaTokenizer\", \"unk_token\": \"\"}**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqhstv08E7sc"
      },
      "source": [
        "**Enter wandb api key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3ZeWhhnE7sc"
      },
      "outputs": [],
      "source": [
        "%pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya2NjlT7BZ2q"
      },
      "source": [
        "**8 x A100 80gb training run** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PUdb3ZY4FkK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!torchrun --nnodes=1 --nproc_per_node=8 --master_port=21001 \\\n",
        "    FastChat/fastchat/train/train.py \\\n",
        "    --model_name_or_path models/decapoda-research_llama-13b-hf \\\n",
        "    --data_path ShareGPT_unfiltered_cleaned_split.json \\\n",
        "    --bf16 True \\\n",
        "    --output_dir ./checkpoints \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 1200 \\\n",
        "    --save_total_limit 100 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --fsdp \"full_shard auto_wrap\" \\\n",
        "    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
        "    --tf32 True \\\n",
        "    --model_max_length 2048 \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --lazy_preprocess True"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}