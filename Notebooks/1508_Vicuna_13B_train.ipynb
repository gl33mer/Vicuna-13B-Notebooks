{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuhausmatheus/vicuna/blob/main/1508_Vicuna_13B_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GNnhOCQE7sa"
      },
      "source": [
        "**You may encounter an error when installing flash-attn. I couldn't figure it out. Maybe you can.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution -1"
      ],
      "metadata": {
        "id": "tKZMmeH5p_nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin\n",
        "%%cuda --name testGoogleColab.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char* file, int line, bool abort = true)\n",
        "{\n",
        "    if (code != cudaSuccess)\n",
        "    {\n",
        "        fprintf(stderr, \"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "        if (abort) exit(code);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void add(int a, int b, int *c) {\n",
        "   *c = a + b; }\n",
        "\n",
        "int main() {\n",
        "\n",
        "   // --- Host declarations and initializations\n",
        "   int a, b, c;\n",
        "   a = 2;\n",
        "   b = 6;\n",
        "\n",
        "   // --- Device allocations\n",
        "   int *d_c; gpuErrchk(cudaMalloc(&d_c, sizeof(int)));\n",
        "\n",
        "   // --- Kernel execution\n",
        "   add<<<1,1>>>(a, b, d_c);\n",
        "   gpuErrchk(cudaPeekAtLastError());\n",
        "   gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "   // --- Moving the results from device to host\n",
        "   gpuErrchk(cudaMemcpy(&c, d_c, sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "   // --- Results printout\n",
        "   printf(\"%d + %d is %d\\n\", a, b, c);\n",
        "   \n",
        "   return 0; }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E-7W0ppwp-NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution 0"
      ],
      "metadata": {
        "id": "frRN7P8mh5Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! add-apt-repository ppa:graphics-drivers/ppa\n",
        "! apt update\n",
        "! apt install nvidia-384 nvidia-384-dev\n",
        "! apt-get install g++ freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libglu1-mesa libglu1-mesa-dev\n",
        "! nvidia-smi\n",
        "!gcc -v\n",
        "!apt install gcc-6\n",
        "!apt install g++-6\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/patches/1/cuda-repo-ubuntu1710-9-2-local-cublas-update-1_1.0-1_amd64\n",
        "!dpkg -i cuda-repo-ubuntu1710-9-2-local_9.2.88-1_amd64\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda\n",
        "!dpkg -i cuda-repo-ubuntu1710-9-2-local-cublas-update-1_1.0-1_amd64\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "Pn0kR6xkh2_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution 1"
      ],
      "metadata": {
        "id": "d4cNBTFdite8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda-!repo-ubuntu1804-11-7-local_11.7.0-515.43.04-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1804-11-7-local_11.7.0-515.43.04-1_amd64.deb\n",
        "!cp /var/cuda-repo-ubuntu1804-11-7-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!apt-get update\n",
        "!apt-get -y install cuda-11-7\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "mf70GZbufAbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution 2 "
      ],
      "metadata": {
        "id": "mcXQtqvBeH76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /usr/local/lib/python3.6/dist-packages/external/local_config_cuda/cuda/cuda/cuda_config.h |\\\n",
        "grep TF_CUDA_VERSION"
      ],
      "metadata": {
        "id": "7hw3xxEueCNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet-cu80\n"
      ],
      "metadata": {
        "id": "6hZqDJCDeFyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution 3"
      ],
      "metadata": {
        "id": "G5KyxNbNfjxg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LF2EEl1_c8Gk"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda\n",
        "!pip install mxnet-cu92\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-TlkbrQXic"
      },
      "source": [
        "Testing GPU access on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d8c7a1-468f-49f9-c7d1-13e6b0318c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr  7 17:07:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    47W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "name of the GPUS:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "!nvcc --version\n",
        "!echo \"name of the GPUS:\"\n",
        "!echo $gpu_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_nO3jaZQhqW"
      },
      "source": [
        "Testing Memory access on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "96b4a911-5fa7-4a98-87e6-8847f33f507c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edefq2tSQV2H"
      },
      "source": [
        "Preparation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h_MevKtB0dEw",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a1c45b-1ceb-47f8-b918-d0ebdb7bfa03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.1+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl (1977.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp39-cp39-linux_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp39-cp39-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1+cu116) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1+cu116) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1+cu116) (1.26.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.1+cu116 torchaudio-0.13.1+cu116 torchvision-0.14.1+cu116\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 135161, done.\u001b[K\n",
            "remote: Counting objects: 100% (248/248), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 135161 (delta 110), reused 212 (delta 103), pack-reused 134913\u001b[K\n",
            "Receiving objects: 100% (135161/135161), 132.72 MiB | 18.03 MiB/s, done.\n",
            "Resolving deltas: 100% (101794/101794), done.\n",
            "Note: switching to 'cae78c46'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at cae78c46d [safetensors] don't use in `torch<1.10` (#22370)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.28.0.dev0) (3.10.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.28.0.dev0) (1.26.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6827740 sha256=2bf301fbbd4d771183ed50f8832db4147804a4806d26e7a56b276bee4194cbe7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c38b_aa2/wheels/1c/6e/db/b90d9f8554f165a9beb90b593fde94e9e60919270aed78efa0\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.0.dev0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "Successfully installed pip-23.0.1\n",
            "Cloning into 'FastChat'...\n",
            "remote: Enumerating objects: 1049, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 1049 (delta 77), reused 115 (delta 50), pack-reused 888\u001b[K\n",
            "Receiving objects: 100% (1049/1049), 28.21 MiB | 18.66 MiB/s, done.\n",
            "Resolving deltas: 100% (625/625), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/FastChat\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-07li7kpt/transformers_b678c567e3434e72926fb48fbce7148a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-07li7kpt/transformers_b678c567e3434e72926fb48fbce7148a\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 117a0f6afa3e19d40cb7d19f645f475244219b71\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fschat==0.1.8) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fschat==0.1.8) (1.22.4)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.1-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.23\n",
            "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from fschat==0.1.8) (1.13.1+cu116)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown2[all]\n",
            "  Downloading markdown2-2.4.8-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.9/dist-packages (from fschat==0.1.8) (0.13.3)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (3.7.1)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (0.13.4)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (8.4.0)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.9-cp39-cp39-manylinux_2_28_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (2.1.2)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (4.5.0)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (6.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (4.2.2)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (1.10.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (2023.3.0)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (2.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio==3.23->fschat==0.1.8) (3.1.2)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate->fschat==0.1.8) (5.9.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate->fschat==0.1.8) (23.0)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wavedrom\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygments>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from markdown2[all]->fschat==0.1.8) (2.14.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fschat==0.1.8) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->fschat==0.1.8) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fschat==0.1.8) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fschat==0.1.8) (2022.12.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->fschat==0.1.8) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->fschat==0.1.8) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->fschat==0.1.8) (3.10.7)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->fschat==0.1.8) (8.1.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb->fschat==0.1.8) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb->fschat==0.1.8) (67.6.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb->fschat==0.1.8) (1.4.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio==3.23->fschat==0.1.8) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio==3.23->fschat==0.1.8) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio==3.23->fschat==0.1.8) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.1.8) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.23->fschat==0.1.8) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio==3.23->fschat==0.1.8) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio==3.23->fschat==0.1.8) (2022.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi->fschat==0.1.8) (3.6.2)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->gradio==3.23->fschat==0.1.8) (22.2.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio==3.23->fschat==0.1.8) (1.3.0)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio==3.23->fschat==0.1.8) (4.39.3)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio==3.23->fschat==0.1.8) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23->fschat==0.1.8) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Building wheels for collected packages: fschat, ffmpy, pathtools, wavedrom\n",
            "  Building editable for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fschat: filename=fschat-0.1.8-0.editable-py3-none-any.whl size=11078 sha256=5daf92b257e47ab5d1b02285125364619462abf0c8f8585f52d9dc4780acfd2c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dwg_crr3/wheels/b1/df/6f/9c508a0c5750a260d14a2cc0bc55a829471df1004dfec1e7f9\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=ac17429803035bad15e9219e1edffb08449ec775253fbc987b054d7d7aa1f3d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=f03cdfd8f0143a2ece0460c3edfb56ddda3400a0bcea85cac0ff6c1fe852c628\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29953 sha256=f04eb153fea6a1bb0fe694bf837ea0949125d9cbecf48b5dae5d7c6885b98f9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\n",
            "Successfully built fschat ffmpy pathtools wavedrom\n",
            "Installing collected packages: sentencepiece, rfc3986, pydub, pathtools, ffmpy, websockets, uc-micro-py, svgwrite, smmap, setproctitle, sentry-sdk, semantic-version, python-multipart, orjson, multidict, markdown2, h11, frozenlist, docker-pycreds, async-timeout, aiofiles, yarl, wavedrom, uvicorn, starlette, mdit-py-plugins, linkify-it-py, httpcore, gitdb, aiosignal, accelerate, httpx, GitPython, fastapi, aiohttp, wandb, gradio, fschat\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.18.0 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 docker-pycreds-0.4.0 fastapi-0.95.0 ffmpy-0.3.0 frozenlist-1.3.3 fschat-0.1.8 gitdb-4.0.10 gradio-3.23.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 linkify-it-py-2.0.0 markdown2-2.4.8 mdit-py-plugins-0.3.3 multidict-6.0.4 orjson-3.8.9 pathtools-0.1.2 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 semantic-version-2.10.0 sentencepiece-0.1.97 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 starlette-0.26.1 svgwrite-1.4.3 uc-micro-py-1.0.1 uvicorn-0.21.1 wandb-0.14.1 wavedrom-2.0.3.post3 websockets-11.0.1 yarl-1.8.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m--2023-04-07 17:11:35--  https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl\n",
            "Resolving huggingface.co (huggingface.co)... 13.224.249.63, 13.224.249.89, 13.224.249.35, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.224.249.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/ea4c106e475c5a8971b107e7d7a4beddaf50e5e1996f2a311c0f6ad81365998b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl%3B+filename%3D%22flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl%22%3B&Expires=1681146695&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4Lzc0LzU4NzRlODIzNGNiY2QzN2RkMzFjYTQ4NmU4NDkyZDlmMTM3MGJkZDA0ODI5MDAxZjUzOTkxYTg2Njg1MWU4M2YvZWE0YzEwNmU0NzVjNWE4OTcxYjEwN2U3ZDdhNGJlZGRhZjUwZTVlMTk5NmYyYTMxMWMwZjZhZDgxMzY1OTk4Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODExNDY2OTV9fX1dfQ__&Signature=YNHcRQHirhCq2i0KwNCd-qbUuxBL8rdj%7Em%7EFSmy9XqCB83GwoF4J12xaHJJVxRiOMDqImeWd9Sz3dOJ90uOuwQWGnXna6AKG19PnJpdPsR4n38dgACG9MY3OXQ6h5rIx1Vjp3rg7yxfnm0JETM5qH0MuXZbFp7q9pdx0tQGPc%7E4xS9-RhJjrGYEriqumqXrnhRpOeD5qRgU8sxPOu7bwJD56KOE7ntTzyF26oHSLQuU5izliWp4AG7N%7EbfktoPw2iP7IimvKWoWHfdVLe8gFFXjty9jkXTcascm4W0jQPMoLyHAAY1ZLeqhc3d2-Mw-E6dQ8K3YNelj1fFbMP2IZkA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-04-07 17:11:35--  https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/ea4c106e475c5a8971b107e7d7a4beddaf50e5e1996f2a311c0f6ad81365998b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl%3B+filename%3D%22flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl%22%3B&Expires=1681146695&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4Lzc0LzU4NzRlODIzNGNiY2QzN2RkMzFjYTQ4NmU4NDkyZDlmMTM3MGJkZDA0ODI5MDAxZjUzOTkxYTg2Njg1MWU4M2YvZWE0YzEwNmU0NzVjNWE4OTcxYjEwN2U3ZDdhNGJlZGRhZjUwZTVlMTk5NmYyYTMxMWMwZjZhZDgxMzY1OTk4Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODExNDY2OTV9fX1dfQ__&Signature=YNHcRQHirhCq2i0KwNCd-qbUuxBL8rdj%7Em%7EFSmy9XqCB83GwoF4J12xaHJJVxRiOMDqImeWd9Sz3dOJ90uOuwQWGnXna6AKG19PnJpdPsR4n38dgACG9MY3OXQ6h5rIx1Vjp3rg7yxfnm0JETM5qH0MuXZbFp7q9pdx0tQGPc%7E4xS9-RhJjrGYEriqumqXrnhRpOeD5qRgU8sxPOu7bwJD56KOE7ntTzyF26oHSLQuU5izliWp4AG7N%7EbfktoPw2iP7IimvKWoWHfdVLe8gFFXjty9jkXTcascm4W0jQPMoLyHAAY1ZLeqhc3d2-Mw-E6dQ8K3YNelj1fFbMP2IZkA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.254.123, 13.227.254.52, 13.227.254.33, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.254.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49942604 (48M) [binary/octet-stream]\n",
            "Saving to: ‘flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl’\n",
            "\n",
            "flash_attn-0.2.8-cp 100%[===================>]  47.63M  12.0MB/s    in 4.0s    \n",
            "\n",
            "2023-04-07 17:11:40 (12.0 MB/s) - ‘flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl’ saved [49942604/49942604]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from flash-attn==0.2.8) (1.13.1+cu116)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (from flash-attn==0.2.8) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->flash-attn==0.2.8) (4.5.0)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-0.2.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!cd ~\n",
        "!git clone https://github.com/huggingface/transformers.git && cd transformers && git checkout cae78c46 && pip install .\n",
        "# Install fastchat\n",
        "!pip3 install --upgrade pip\n",
        "!git clone https://github.com/lm-sys/FastChat && cd FastChat && pip install -e .\n",
        "%pip install einops\n",
        "!mkdir checkpoints\n",
        "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl\n",
        "%pip install flash_attn-0.2.8-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6SNhHJFz-28c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed615c1d-9951-4110-f225-5f14ac992f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-07 17:12:03--  https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/download-model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9424 (9.2K) [text/plain]\n",
            "Saving to: ‘download-model.py’\n",
            "\n",
            "download-model.py   100%[===================>]   9.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-07 17:12:04 (98.3 MB/s) - ‘download-model.py’ saved [9424/9424]\n",
            "\n",
            "--2023-04-07 17:12:04--  https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V2_unfiltered_cleaned_split.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.224.249.63, 13.224.249.89, 13.224.249.35, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.224.249.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/fdce6e0f96d37c6b5c0c2b3aeb63172283bbd40818f267009e87399a59764221?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ShareGPT_V2_unfiltered_cleaned_split.json%3B+filename%3D%22ShareGPT_V2_unfiltered_cleaned_split.json%22%3B&response-content-type=application%2Fjson&Expires=1681146725&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4Lzc0LzU4NzRlODIzNGNiY2QzN2RkMzFjYTQ4NmU4NDkyZDlmMTM3MGJkZDA0ODI5MDAxZjUzOTkxYTg2Njg1MWU4M2YvZmRjZTZlMGY5NmQzN2M2YjVjMGMyYjNhZWI2MzE3MjI4M2JiZDQwODE4ZjI2NzAwOWU4NzM5OWE1OTc2NDIyMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODExNDY3MjV9fX1dfQ__&Signature=hmxtPaoZAWVuKjMOC4F8vycJVmsb4WMEWWHYAD-J5lxkRr7dH6NMjpxOX2cmC7wYM5QpeHA4ymLpMpz1cPf4QFIPECoAXA1xaevnu4bUz1cBpGd%7EEAWc6upAyO%7Eln-kK9wCiI40eMlZ20pAAHAXCHzjysCu8rkzRnwOntQtAd%7EC09kKRJPrnIgldEHgRXZbqtD8pXTtBoruWou3dFVr7X%7EtiqYYr%7EltNQK9-O1Z2FljAZ8RI771Uu%7E6yp0zfgVfU-5euECUcDpMc0erDLYU1De5of%7Ell3RgsqU1skft8KpApuZpRHfhtI%7EoYzG2HbjPvbcYAU2n-ImeJe6veCSm6yg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-04-07 17:12:04--  https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/fdce6e0f96d37c6b5c0c2b3aeb63172283bbd40818f267009e87399a59764221?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ShareGPT_V2_unfiltered_cleaned_split.json%3B+filename%3D%22ShareGPT_V2_unfiltered_cleaned_split.json%22%3B&response-content-type=application%2Fjson&Expires=1681146725&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU4Lzc0LzU4NzRlODIzNGNiY2QzN2RkMzFjYTQ4NmU4NDkyZDlmMTM3MGJkZDA0ODI5MDAxZjUzOTkxYTg2Njg1MWU4M2YvZmRjZTZlMGY5NmQzN2M2YjVjMGMyYjNhZWI2MzE3MjI4M2JiZDQwODE4ZjI2NzAwOWU4NzM5OWE1OTc2NDIyMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODExNDY3MjV9fX1dfQ__&Signature=hmxtPaoZAWVuKjMOC4F8vycJVmsb4WMEWWHYAD-J5lxkRr7dH6NMjpxOX2cmC7wYM5QpeHA4ymLpMpz1cPf4QFIPECoAXA1xaevnu4bUz1cBpGd%7EEAWc6upAyO%7Eln-kK9wCiI40eMlZ20pAAHAXCHzjysCu8rkzRnwOntQtAd%7EC09kKRJPrnIgldEHgRXZbqtD8pXTtBoruWou3dFVr7X%7EtiqYYr%7EltNQK9-O1Z2FljAZ8RI771Uu%7E6yp0zfgVfU-5euECUcDpMc0erDLYU1De5of%7Ell3RgsqU1skft8KpApuZpRHfhtI%7EoYzG2HbjPvbcYAU2n-ImeJe6veCSm6yg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.254.123, 13.227.254.52, 13.227.254.33, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.254.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 578051903 (551M) [application/json]\n",
            "Saving to: ‘ShareGPT_V2_unfiltered_cleaned_split.json’\n",
            "\n",
            "ShareGPT_V2_unfilte 100%[===================>] 551.27M  18.1MB/s    in 32s     \n",
            "\n",
            "2023-04-07 17:12:37 (17.2 MB/s) - ‘ShareGPT_V2_unfiltered_cleaned_split.json’ saved [578051903/578051903]\n",
            "\n",
            "Downloading the model to models/decapoda-research_llama-13b-hf\n",
            "100% 8.31k/8.31k [00:00<00:00, 8.24MiB/s]\n",
            "100% 427/427 [00:00<00:00, 453kiB/s]\n",
            "100% 124/124 [00:00<00:00, 124kiB/s]\n",
            "100% 952M/952M [00:09<00:00, 104MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.4MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 84.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.0MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 82.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 86.4MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 86.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 84.3MiB/s]\n",
            "100% 952M/952M [01:07<00:00, 14.2MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.1MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.2MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 83.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 82.5MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 84.7MiB/s]\n",
            "100% 952M/952M [00:13<00:00, 69.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 83.0MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 84.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 86.4MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 86.5MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 86.5MiB/s]\n",
            "100% 952M/952M [00:09<00:00, 95.4MiB/s]\n",
            "100% 952M/952M [00:09<00:00, 99.8MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 87.1MiB/s]\n",
            "100% 952M/952M [00:12<00:00, 78.3MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 83.9MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.5MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 86.8MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 87.3MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 84.3MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.8MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 86.3MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.3MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 88.7MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 92.4MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 86.8MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 93.6MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.8MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 87.7MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 87.6MiB/s]\n",
            "100% 952M/952M [00:11<00:00, 85.6MiB/s]\n",
            "100% 952M/952M [00:10<00:00, 87.5MiB/s]\n",
            "100% 983M/983M [00:11<00:00, 83.5MiB/s]\n",
            "100% 31.8k/31.8k [00:00<00:00, 22.7MiB/s]\n",
            "100% 2.00/2.00 [00:00<00:00, 1.86kiB/s]\n",
            "100% 500k/500k [00:00<00:00, 46.0MiB/s]\n",
            "100% 141/141 [00:00<00:00, 133kiB/s]\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/oobabooga/text-generation-webui/main/download-model.py\n",
        "!mkdir models\n",
        "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V2_unfiltered_cleaned_split.json\n",
        "!python download-model.py decapoda-research/llama-13b-hf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIkKAeiqE7sb"
      },
      "source": [
        "**Manually edit tokenizer_config.json to: {\"bos_token\": \"\", \"eos_token\": \"\", \"model_max_length\": 2048, \"tokenizer_class\": \"LlamaTokenizer\", \"unk_token\": \"\"}**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqhstv08E7sc"
      },
      "source": [
        "**Enter wandb api key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3ZeWhhnE7sc"
      },
      "outputs": [],
      "source": [
        "%pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya2NjlT7BZ2q"
      },
      "source": [
        "**8 x A100 80gb training run**  (CUDA_VISIBLE_DEVICES=0 && NVIDIA_VISIBLE_DEVICES=$gpu_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename: ShareGPT_unfiltered_cleaned_split.json or     --data_path ShareGPT_unfiltered_cleaned_split.json \\\n"
      ],
      "metadata": {
        "id": "8R924sAIzd0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the batch size: One of the easiest ways to reduce GPU memory usage is by reducing the batch size. In the code you provided, you can try reducing the per_device_train_batch_size and per_device_eval_batch_size arguments to a smaller value. However, this may increase the training time since it takes more iterations to cover the entire dataset.\n",
        "\n",
        "Use mixed precision training: Another way to reduce GPU memory usage is by using mixed precision training. This involves using lower precision floating-point numbers (e.g., float16 instead of float32) for some parts of the computation, which reduces memory usage and can speed up training. In the code you provided, you can try setting the bf16 and tf32 arguments to False to disable mixed precision training.\n",
        "\n",
        "Use gradient accumulation: If reducing the batch size is not an option, you can try using gradient accumulation. This involves accumulating the gradients over multiple iterations before updating the model parameters, which reduces the memory usage per iteration. In the code you provided, you can try increasing the gradient_accumulation_steps argument to a larger value (e.g., 2, 4, etc.).\n",
        "\n",
        "Use smaller models: If none of the above options work, you can try using a smaller model. In the code you provided, you can try using a smaller pre-trained model by setting the model_name_or_path argument to a smaller model (e.g., \"distilgpt2\" instead of \"decapoda-research_llama-13b-hf\"). However, this may reduce the model's performance on the task"
      ],
      "metadata": {
        "id": "LMZsUX0Z4-cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9PUdb3ZY4FkK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9fe39a-c438-4a4c-d643-f3977047fe37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "2023-04-07 18:17:26.347634: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/training_args.py:1356: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 41/41 [02:35<00:00,  3.79s/it]\n",
            "Using pad_token, but it is not set yet.\n",
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...Skip in lazy mode\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/FastChat/fastchat/train/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m340\u001b[0m in \u001b[92m<module>\u001b[0m                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m338 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m339 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m340 \u001b[2m│   \u001b[0mtrain()                                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/FastChat/fastchat/train/\u001b[0m\u001b[1;33mtrain.py\u001b[0m:\u001b[94m333\u001b[0m in \u001b[92mtrain\u001b[0m                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m330 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlist\u001b[0m(pathlib.Path(training_args.output_dir).glob(\u001b[33m\"\u001b[0m\u001b[33mcheckpoint-*\u001b[0m\u001b[33m\"\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m331 \u001b[0m\u001b[2m│   │   \u001b[0mtrainer.train(resume_from_checkpoint=\u001b[94mTrue\u001b[0m)                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m332 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m333 \u001b[2m│   │   \u001b[0mtrainer.train()                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m334 \u001b[0m\u001b[2m│   \u001b[0mtrainer.save_state()                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2m│   \u001b[0msafe_save_model_for_hf_trainer(trainer=trainer,                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │      \u001b[0moutput_dir=training_args.output_dir \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1644\u001b[0m in \u001b[92mtrain\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1641 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1644 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1645 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1731\u001b[0m in       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1728 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m args.gradient_checkpointing:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1729 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.model.gradient_checkpointing_enable()                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1730 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1731 \u001b[2m│   │   \u001b[0mmodel = \u001b[96mself\u001b[0m._wrap_model(\u001b[96mself\u001b[0m.model_wrapped)                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1732 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1733 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m is_sagemaker_mp_enabled() \u001b[95mand\u001b[0m resume_from_checkpoint \u001b[95mis\u001b[0m \u001b[95mno\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1734 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._load_from_checkpoint(resume_from_checkpoint, model) \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1469\u001b[0m in       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_wrap_model\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1466 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmixed_precision_policy = MixedPrecision(param_dty \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1467 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mtype\u001b[0m(model) != FSDP:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1468 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# XXX: Breaking the self.model convention but I s\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1469 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.model = model = FSDP(                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1470 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mmodel,                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1471 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0msharding_strategy=\u001b[96mself\u001b[0m.fsdp,                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1472 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mcpu_offload=cpu_offload,                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mfully_sharded_\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mdata_parallel.py\u001b[0m:\u001b[94m1036\u001b[0m in \u001b[92m__init__\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1033 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mforward_prefetch\u001b[0m\u001b[33m\"\u001b[0m: forward_prefetch,                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1034 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mlimit_all_gathers\u001b[0m\u001b[33m\"\u001b[0m: limit_all_gathers,               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1035 \u001b[0m\u001b[2m│   │   │   \u001b[0m}                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1036 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._auto_wrap(auto_wrap_kwargs, fsdp_kwargs)            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1037 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1038 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.process_group = process_group \u001b[95mor\u001b[0m _get_default_group()    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1039 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.rank = \u001b[96mself\u001b[0m.process_group.rank()                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mfully_sharded_\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mdata_parallel.py\u001b[0m:\u001b[94m1291\u001b[0m in \u001b[92m_auto_wrap\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1288 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mkernels do not support low precision.\u001b[0m\u001b[33m\"\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1289 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1290 \u001b[0m\u001b[2m│   │   │   \u001b[0mauto_wrap_kwargs[\u001b[33m\"\u001b[0m\u001b[33mauto_wrap_policy\u001b[0m\u001b[33m\"\u001b[0m] = auto_wrap_policy   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1291 \u001b[2m│   │   \u001b[0m_recursive_wrap(**auto_wrap_kwargs, **fsdp_kwargs)            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1292 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1293 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_check_single_device_module\u001b[0m(                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1294 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mwrap.py\u001b[0m:\u001b[94m403\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_recursive_wrap\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, child \u001b[95min\u001b[0m module.named_children():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m child \u001b[95min\u001b[0m ignored_modules:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   │   │   \u001b[0mwrapped_child, num_wrapped_params = _recursive_wrap(       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodule=child,                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mauto_wrap_policy=auto_wrap_policy,                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapper_cls=wrapper_cls,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mwrap.py\u001b[0m:\u001b[94m403\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_recursive_wrap\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, child \u001b[95min\u001b[0m module.named_children():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m child \u001b[95min\u001b[0m ignored_modules:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   │   │   \u001b[0mwrapped_child, num_wrapped_params = _recursive_wrap(       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodule=child,                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mauto_wrap_policy=auto_wrap_policy,                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapper_cls=wrapper_cls,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mwrap.py\u001b[0m:\u001b[94m403\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_recursive_wrap\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m400 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, child \u001b[95min\u001b[0m module.named_children():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m401 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m child \u001b[95min\u001b[0m ignored_modules:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m402 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mcontinue\u001b[0m                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m403 \u001b[2m│   │   │   \u001b[0mwrapped_child, num_wrapped_params = _recursive_wrap(       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m404 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodule=child,                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m405 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mauto_wrap_policy=auto_wrap_policy,                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m406 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapper_cls=wrapper_cls,                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mwrap.py\u001b[0m:\u001b[94m421\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_recursive_wrap\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m418 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodule=module, recurse=\u001b[94mFalse\u001b[0m, unwrapped_params=remainder   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m419 \u001b[0m\u001b[2m│   │   \u001b[0m):                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m420 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Leaf node or final wrapping of the remainder both happen\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m421 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _wrap(module, wrapper_cls, **kwargs), num_params    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m422 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m423 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m module, total_wrapped_params                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m424 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m module, \u001b[94m0\u001b[0m                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mwrap.py\u001b[0m:\u001b[94m350\u001b[0m in \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_wrap\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m347 \u001b[0m\u001b[2m│   │   \u001b[0moverrides = {**kwargs, **module._wrap_overrides}  \u001b[2m# type: igno\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m348 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper_cls(module, **overrides)                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m349 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m350 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m wrapper_cls(module, **kwargs)                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m351 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m352 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m353 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_recursive_wrap\u001b[0m(                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mfully_sharded_\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33mdata_parallel.py\u001b[0m:\u001b[94m1079\u001b[0m in \u001b[92m__init__\u001b[0m                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1076 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.mixed_precision.reduce_dtype,                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1077 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.mixed_precision.keep_low_precision_grads,            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1078 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1079 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._fsdp_wrapped_module = FlattenParamsWrapper(             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1080 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodule,                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1081 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams_to_flatten,                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1082 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.compute_device,                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mflatten_params\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[1;33m_wrapper.py\u001b[0m:\u001b[94m103\u001b[0m in \u001b[92m__init__\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._register_load_state_dict_pre_hook(_pre_load_state_dict_h \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(params) == \u001b[94m0\u001b[0m:                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m103 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._flat_param_handle = FlatParamHandle(params, module, devi \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Defining `self.flat_param` registers the `FlatParameter` and\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# visible to `named_parameters()`\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.flat_param = \u001b[96mself\u001b[0m._flat_param_handle.flat_param           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mflat_param.py\u001b[0m: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m270\u001b[0m in \u001b[92m__init__\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 267 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.device = device                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 268 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._config = config                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 269 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._training_state = HandleTrainingState.IDLE               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 270 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._init_flat_param(params, module)                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 271 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._unflatten(as_params=\u001b[94mFalse\u001b[0m)                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 272 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 273 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_init_flat_param\u001b[0m(                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mflat_param.py\u001b[0m: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m357\u001b[0m in \u001b[92m_init_flat_param\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 354 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 355 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprefixed_param_names.append(prefixed_param_name)  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 356 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m requires_grad \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 357 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.flat_param = FlatParamHandle.flatten_params(             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 358 \u001b[0m\u001b[2m│   │   │   \u001b[0mparams_to_flatten, requires_grad                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 359 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 360 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.flat_param._init_metadata(                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/torch/distributed/fsdp/\u001b[0m\u001b[1;33mflat_param.py\u001b[0m: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m388\u001b[0m in \u001b[92mflatten_params\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 385 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mp.detach().reshape(-\u001b[94m1\u001b[0m) \u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(p, nn.Parameter) \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 386 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m p \u001b[95min\u001b[0m params                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 387 \u001b[0m\u001b[2m│   │   │   \u001b[0m]                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 388 \u001b[2m│   │   │   \u001b[0mflat_param_data = torch.cat(flat_params, dim=\u001b[94m0\u001b[0m)           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 389 \u001b[0m\u001b[2m│   │   \u001b[0mflat_param = FlatParameter(flat_param_data, requires_grad=req \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 390 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m flat_param                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 391 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m1.18\u001b[0m GiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m39.56\u001b[0m \n",
            "GiB total capacity; \u001b[1;36m37.88\u001b[0m GiB already allocated; \u001b[1;36m430.56\u001b[0m MiB free; \u001b[1;36m37.94\u001b[0m GiB \n",
            "reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
            "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \n",
            "Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 19119) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 762, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 753, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "FastChat/fastchat/train/train.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-04-07_18:22:22\n",
            "  host      : 98e7376f5e58\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 19119)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1 && echo $CUDA_LAUNCH_BLOCKING && unset CUDA_VISIBLE_DEVICES && export CUDA_VISIBLE_DEVICES=0 && echo $CUDA_VISIBLE_DEVICES && \\\n",
        "torchrun --nnodes=1 --nproc_per_node=1 --master_port=21001 \\\n",
        "    FastChat/fastchat/train/train.py \\\n",
        "    --model_name_or_path models/decapoda-research_llama-13b-hf \\\n",
        "    --data_path ShareGPT_unfiltered_cleaned_split.json \\\n",
        "    --bf16 False \\\n",
        "    --output_dir ./checkpoints \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 1200 \\\n",
        "    --save_total_limit 100 \\\n",
        "    --learning_rate 4e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --fsdp \"full_shard auto_wrap\" \\\n",
        "    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
        "    --tf32 False \\\n",
        "    --model_max_length 1024 \\\n",
        "    --gradient_checkpointing False \\\n",
        "    --lazy_preprocess True"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}